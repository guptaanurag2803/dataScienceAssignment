{
 "cells": [
  {
   "cell_type": "raw",
   "id": "af40f50e",
   "metadata": {},
   "source": [
    "#1.\n",
    "\n",
    "Gradient Boosting Regression:\n",
    "\n",
    "1. Ensemble Technique: Gradient Boosting Regression is an ensemble method for regression tasks.\n",
    "2. Weak Learners: It combines multiple weak learners, often decision trees, into a strong predictive model.\n",
    "3. Residuals: It calculates the differences between actual target values and initial predictions (residuals).\n",
    "4. Iterative Refinement: Weak learners are trained to predict residuals, iteratively improving predictions with each step.\n",
    "5. Gradient Descent: The algorithm uses gradient descent optimization to minimize prediction errors.\n",
    "6. Learning Rate: A learning rate controls the influence of each weak learner's prediction, aiding stability.\n",
    "7. Accurate Predictions: Each weak learner corrects errors of the previous ones, leading to more accurate predictions.\n",
    "8. Complex Patterns: Gradient Boosting Regression excels in capturing complex nonlinear relationships in data.\n",
    "9. Hyperparameter Tuning: Proper tuning is crucial to prevent overfitting and control learning rates.\n",
    "10. Final Prediction: The ensemble prediction is the cumulative sum of initial predictions and weighted predictions from all weak learners.\n",
    "11. Implementation Variants: Implementations include scikit-learn's `GradientBoostingRegressor`, XGBoost, LightGBM, and CatBoost.\n",
    "12. Performance: It's effective for handling outliers, but its success depends on careful parameter selection and data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc36a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error:  32.82456734895318\n",
      "R-squared:  -0.00044427694902005044\n"
     ]
    }
   ],
   "source": [
    "#2.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(50, 1) * 10\n",
    "y = 2 * X + 1 + np.random.randn(50, 1)\n",
    "\n",
    "n_estimators = 100\n",
    "\n",
    "y_pred = np.full_like(y, np.mean(y))\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    residuals = y - y_pred\n",
    "    weak_learner = np.random.randn(50, 1)\n",
    "    y_pred += 0.1 * weak_learner\n",
    "    \n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "print(\"R-squared: \", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1def6320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 2, 'n_estimators': 50}\n",
      "Best Mean Squared Error: 31.0570\n"
     ]
    }
   ],
   "source": [
    "#3.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(50, 1) * 10\n",
    "y = 2 * X + 1 + np.random.randn(50, 1)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [1, 2, 3]\n",
    "}\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    n_estimators = params['n_estimators']\n",
    "    learning_rate = params['learning_rate']\n",
    "    max_depth = params['max_depth']\n",
    "\n",
    "    y_pred = np.full_like(y, np.mean(y))\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        residuals = y - y_pred\n",
    "        weak_learner = np.random.randn(50, 1)\n",
    "        y_pred += learning_rate * weak_learner\n",
    "\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    \n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Mean Squared Error: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06275277",
   "metadata": {},
   "source": [
    "#4.\n",
    "\n",
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on a given task. It's typically a decision tree with limited depth, a small number of nodes, or constrained features. The concept of a weak learner is integral to boosting algorithms, as the ensemble's strength arises from combining multiple instances of these learners.\n",
    "\n",
    "Weak learners are intentionally designed to have limited predictive power, often focusing on a specific aspect of the data. They may struggle to accurately capture complex relationships in the data, but when combined through boosting, their collective performance improves. The iterative nature of boosting emphasizes instances that are misclassified by previous learners, allowing subsequent weak learners to focus on these challenging cases and refine the ensemble's overall prediction. This iterative process transforms weak learners into a strong ensemble that excels in making accurate predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e50a02c",
   "metadata": {},
   "source": [
    "#5.\n",
    "\n",
    "The intuition behind the Gradient Boosting algorithm is to iteratively correct the errors of weak learners by building a strong ensemble model. It's like assembling a team of experts, where each expert specializes in fixing a certain aspect of a problem. Here's a more detailed intuition:\n",
    "\n",
    "Learning from Mistakes: The algorithm starts with an initial prediction for all instances. In each iteration, it focuses on instances that were mispredicted, indicating where the current ensemble is struggling.\n",
    "\n",
    "Building in Steps: Weak learners, often simple decision trees, are trained to predict the errors (residuals) of the current ensemble. Each weak learner adds its expertise in correcting specific errors.\n",
    "\n",
    "Iterative Refinement: Weak learners are added sequentially, with each one concentrating on correcting the mistakes of the previous ensemble. The algorithm adjusts the weights of instances, emphasizing misclassified ones.\n",
    "\n",
    "Collective Wisdom: As iterations progress, the ensemble's accuracy improves because it learns from the collective insights of weak learners, each addressing a unique challenge.\n",
    "\n",
    "Complex Patterns: The ensemble gradually adapts to complex relationships in the data, effectively capturing nuances that individual weak learners might miss.\n",
    "\n",
    "Regularization: The learning rate parameter controls the contribution of each weak learner, preventing rapid overfitting and promoting more cautious learning.\n",
    "\n",
    "Robustness: The ensemble becomes robust by aggregating the strengths of different learners, reducing the impact of noise and outliers.\n",
    "\n",
    "Final Ensemble: The final ensemble prediction is the sum of initial predictions and the weighted contributions of all weak learners. It represents a powerful, accurate model."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c16167ec",
   "metadata": {},
   "source": [
    "#6.\n",
    "\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative and sequential manner. Each weak learner is designed to correct the errors made by the previous ensemble. Here's how the algorithm builds the ensemble:\n",
    "\n",
    "Initialization: The algorithm starts with an initial prediction for all instances. This initial prediction can be a simple value like the mean of the target variable.\n",
    "\n",
    "Compute Residuals: Residuals are the differences between the true target values and the current ensemble's predictions. In the beginning, these residuals are the same as the initial predictions.\n",
    "\n",
    "Train Weak Learner: A weak learner, often a decision tree with limited depth, is trained on the dataset. However, the target values for this training are not the actual target values; instead, they are the residuals from the previous step.\n",
    "\n",
    "Compute Learner's Predictions: The weak learner's predictions are obtained using the features in the dataset.\n",
    "\n",
    "Update Ensemble's Predictions: The predictions made by the weak learner are scaled by a learning rate and added to the ensemble's predictions from the previous iteration. This step refines the overall prediction by reducing the error on instances where the previous ensemble struggled.\n",
    "\n",
    "Adjust Weights: Instances that were misclassified by the ensemble in the previous iteration are assigned higher weights. This emphasizes challenging cases in the training of the next weak learner.\n",
    "\n",
    "Iteration: Steps 2 to 6 are repeated for a specified number of iterations, with each iteration adding a new weak learner to the ensemble.\n",
    "\n",
    "Final Ensemble Prediction: The final prediction of the Gradient Boosting ensemble is the sum of the initial prediction and the weighted predictions of all the weak learners. This ensemble prediction represents the model's final output."
   ]
  },
  {
   "cell_type": "raw",
   "id": "00b4425d",
   "metadata": {},
   "source": [
    "#7.\n",
    "\n",
    "Building the mathematical intuition of Gradient Boosting involves these steps:\n",
    "\n",
    "1. Objective Function: Define a function measuring difference between predictions and targets (e.g., mean squared error for regression).\n",
    "\n",
    "2. Initial Prediction: Start with a basic prediction for all instances (e.g., mean of target values).\n",
    "\n",
    "3. Residuals: Calculate the differences between actual targets and initial predictions.\n",
    "\n",
    "4. Weak Learner Training: Train a simple model (e.g., shallow decision tree) on the residuals to minimize their error.\n",
    "\n",
    "5. Update Predictions: Adjust ensemble predictions by adding weak learner's predictions, scaled by a learning rate.\n",
    "\n",
    "6. Gradient Descent: Like gradient descent, weak learner updates move predictions in direction that minimizes objective function.\n",
    "\n",
    "7. Learning Rate: Controls step size in gradient descent, affecting update magnitude.\n",
    "\n",
    "8. Weight Adjustment: Increase weights of misclassified instances to emphasize their importance.\n",
    "\n",
    "9. Iteration: Repeat steps 3-8 for iterations, with each weak learner correcting previous errors.\n",
    "\n",
    "10. Final Prediction: Ensemble's final prediction is cumulative sum of initial prediction and weighted weak learner predictions.\n",
    "\n",
    "11. Objective Function Minimization: Iteratively updating predictions and introducing learners aims to minimize the objective function, leading to an accurate predictive model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
