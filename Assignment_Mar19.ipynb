{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18a7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "\n",
    "# Min-Max scaling is a data normalization technique used in data preprocessing.\n",
    "# It rescales the data to a specified range, typically between 0 and 1 or -1 and 1.\n",
    "# It is achieved by subtracting the minimum value and dividing by the range of the data.\n",
    "\n",
    "# Formula can be given as:\n",
    "# (value - min_value)/(max_value - min_value)\n",
    "\n",
    "# where,\n",
    "# value = value which is to be normalized\n",
    "# min_value = minimum value of the feature\n",
    "# max_value = maximum value of the feature\n",
    "\n",
    "# For example, consider a dataset with features such as age (ranging from 20 to 60) and income (ranging from 20,000 to 100,000).\n",
    "# By applying Min-Max scaling, both features can be transformed to a common range (e.g., 0 to 1).\n",
    "# This scaling ensures that features with larger numeric ranges do not dominate the learning process.\n",
    "# Consequently, the scaled data can improve the performance of machine learning algorithms that are sensitive to the scale of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd0d2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "# The Unit Vector technique, or vector normalization, is a feature scaling method that scales data to have a unit norm or length of 1.\n",
    "# It involves dividing each data point by the Euclidean norm of the feature vector.\n",
    "# This ensures all vectors have the same scale and direction.\n",
    "# Unlike Min-Max scaling, which transforms data to a specific range, Unit Vector scaling emphasizes the relative importance of each feature rather than the absolute values.\n",
    "\n",
    "# For example, in a dataset with features like age, income, and education level.\n",
    "# Unit Vector scaling would normalize each feature vector to a unit length, making them comparable in terms of their direction and magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592d3c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.\n",
    "\n",
    "# Principal Component Analysis, is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space while retaining the most important information.\n",
    "# It identifies the principal components, which are new orthogonal axes that capture the maximum variance in the data.\n",
    "# These components are ordered by their significance, and the lower-dimensional representation can be achieved by selecting the top components.\n",
    "\n",
    "# For example, given a dataset with multiple correlated features such as age, income, and education level.\n",
    "# PCA can be used to extract the most influential components that explain the majority of the variance in the data, reducing the dimensionality while preserving the most informative aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0b54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.\n",
    "\n",
    "# PCA plays a significant role in feature extraction.\n",
    "# It can be used as a feature extraction technique to transform high-dimensional data into a lower-dimensional representation by identifying the most important features.\n",
    "# By selecting the top principal components, which capture the maximum variance in the data, PCA effectively extracts the most informative features from the original dataset.\n",
    "# This reduced feature set can then be used for various tasks such as classification or clustering.\n",
    "\n",
    "# For example, in face recognition, PCA can extract key facial features from images, representing them with a lower-dimensional set of principal components.\n",
    "# Principal components which can be used for subsequent analysis or classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7d9b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "\n",
    "# To preprocess the data for building a recommendation system, Min-Max scaling can be used on the features of price, rating, and delivery time.\n",
    "# By applying Min-Max scaling, each feature's values will be transformed to a common range, typically between 0 and 1.\n",
    "# This ensures that all features are on the same scale, preventing one feature from dominating the others.\n",
    "# It allows for fair comparisons and enables the recommendation system to consider the relative importance of each feature when making recommendations.\n",
    "\n",
    "# For example, a high price would not outweigh other important factors like rating and delivery time, resulting in a more balanced and effective recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36f847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.\n",
    "\n",
    "# To reduce the dimensionality of the dataset for predicting stock prices, PCA can be applied.\n",
    "# Then, PCA is used to extract the most significant components that capture the maximum variance in the data.\n",
    "# By selecting a lower number of principal components, the dimensionality is reduced while retaining the most informative aspects of the data.\n",
    "# This helps in reducing noise and redundancy, improving computational efficiency, and mitigating the curse of dimensionality.\n",
    "# It ultimately aiding in building a more efficient and accurate stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d35da53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#7.\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "min_max = MinMaxScaler(feature_range = (-1, 1))\n",
    "min_max.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af7b8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.\n",
    "\n",
    "# Perform PCA on the dataset with features [height, weight, age, gender, blood pressure].\n",
    "# Calculate the explained variance ratio for each principal component.\n",
    "# Choose the number of principal components that collectively explain a significant portion of the variance, such as 90% or 95%.\n",
    "# This ensures a balance between dimensionality reduction and information retention.\n",
    "# The specific number of principal components to retain depends on the desired level of information retention and the trade-off between dimensionality reduction and predictive accuracy.\n",
    "# It is important to consider the specific requirements and constraints of the project when deciding on the number of principal components to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
