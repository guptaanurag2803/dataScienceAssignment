{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a6e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "\n",
    "# Simple Linear Regression:\n",
    "# In Linear regression algorithm, there is a single independent variable and single dependent variable.\n",
    "# Both independent variable and dependent variable are continuous.\n",
    "# There is a linear relationship between independent variable and dependent variable.\n",
    "# The model equation for simple linear regression is:\n",
    "# hθ(x) = θ0 + θ1x1\n",
    "# e.g. When we predict the height of a person using his weight.\n",
    "# There is only one independent variable, weight and one dependent variable, height.\n",
    "\n",
    "# Multiple Linear Regression:\n",
    "# In Linear regression algorithm, there is are multiple independent variable and single dependent variable.\n",
    "# All independent variable and dependent variable are continuous.\n",
    "# There is a linear relationship between each independent variable and dependent variable.\n",
    "# The model equation for multiple linear regression is:\n",
    "# hθ(x) = θ0 + θ1x1 + θ2x2 + ... + θnxn\n",
    "# e.g. When we predict the price of a house using his no. of rooms and area.\n",
    "# There is only two independent variable, no. of rooms & area and one dependent variable, price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07382f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "# Linear regression relies on several assumptions for accurate and reliable results:\n",
    "\n",
    "# Linearity:\n",
    "# The relationship between the dependent variable and independent variables is assumed to be linear.\n",
    "\n",
    "# Independence:\n",
    "# The observations are independent of each other.\n",
    "    \n",
    "# Homoscedasticity:\n",
    "# The variance of the errors is constant across all levels of the independent variables.\n",
    "    \n",
    "# Normality:\n",
    "# The errors follow a normal distribution with mean zero.\n",
    "    \n",
    "# No multicollinearity:\n",
    "# The independent variables are not highly correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df595d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.\n",
    "\n",
    "# In a linear regression model, the slope and intercept provide valuable insights into the relationship between the dependent variable and the independent variable(s).\n",
    "\n",
    "# The slope represents the change in the dependent variable for a one-unit increase in the independent variable, assuming all other variables are held constant.\n",
    "# It indicates the rate or magnitude of change in the dependent variable per unit change in the independent variable.\n",
    "# A positive slope implies a positive relationship, while a negative slope indicates a negative relationship.\n",
    "\n",
    "# The intercept represents the estimated value of the dependent variable when all independent variables are zero.\n",
    "# It provides the starting point or the value of the dependent variable when the independent variable(s) have no impact.\n",
    "\n",
    "# For example, in a real-world scenario, let's consider a linear regression model that examines the relationship between years of work experience (X) and salary (Y).\n",
    "# The slope coefficient (e.g., θ1 = 2000) indicates that, on average, for each additional year of work experience, the salary increases by $2000, assuming all other factors remain constant.\n",
    "# The intercept (e.g., θ0 = 30000) suggests that a person with zero years of work experience would have an estimated starting salary of $30,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1952b8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.\n",
    "\n",
    "# Gradient descent is an optimization algorithm used in machine learning to iteratively minimize the cost function of a model.\n",
    "# The goal is to find the optimal values for the model's parameters that minimize the difference between predicted and actual values. \n",
    "\n",
    "# The concept of gradient descent involves taking steps in the direction of steepest descent along the negative gradient of the cost function.\n",
    "# At each iteration, the algorithm calculates the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "# It then adjusts the parameter values by subtracting a fraction of the gradient, known as the learning rate, multiplied by the gradient.\n",
    "\n",
    "# By repeatedly updating the parameter values, the algorithm descends down the gradient, approaching the minimum of the cost function.\n",
    "# This process continues until convergence, where further iterations yield minimal improvement or the algorithm reaches a predefined stopping criterion.\n",
    "\n",
    "# Gradient descent is widely used in machine learning, particularly for training models such as linear regression, logistic regression, and neural networks.\n",
    "# It enables these models to learn from data and optimize their parameters to make accurate predictions or classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436fbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "\n",
    "# Multiple linear regression is a statistical model that examines the relationship between a dependent variable and multiple independent variables.\n",
    "# It extends the concept of simple linear regression, which only considers one independent variable.\n",
    "# In multiple linear regression, the model equation takes the form:\n",
    "\n",
    "# hθ(x) = θ0 + θ1x1 + θ2x2 + ... + θnxn\n",
    "\n",
    "# Here, hθ(x) represents the dependent variable, x1, x2, ... , xn represent the independent variables, θ0 is the intercept, θ1, θ2, ..., θn are the coefficients for the independent variables.\n",
    "\n",
    "# The key difference between multiple linear regression and simple linear regression lies in the number of independent variables involved.\n",
    "# Simple linear regression examines the relationship between a dependent variable and a single independent variable, assuming a linear relationship.\n",
    "# Multiple linear regression, on the other hand, incorporates multiple independent variables, allowing for the analysis of their combined impact on the dependent variable.\n",
    "# This enables the model to account for more complex relationships and potentially better predict the dependent variable based on the influence of multiple factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "767db46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.\n",
    "\n",
    "# Multicollinearity refers to a situation in multiple linear regression where two or more independent variables are highly correlated with each other.\n",
    "# It can cause issues in the regression model, making it challenging to determine the unique impact of each independent variable on the dependent variable.\n",
    "\n",
    "# To detect multicollinearity, one common approach is to calculate the correlation matrix between the independent variables.\n",
    "# High correlation coefficients (close to +1 or -1) indicate potential multicollinearity.\n",
    "# Additionally, variance inflation factors (VIF) can be calculated, with higher VIF values suggesting higher levels of multicollinearity.\n",
    "\n",
    "# To address multicollinearity, several strategies can be employed:\n",
    "# 1. Remove one or more highly correlated independent variables from the model.\n",
    "# 2. Combine or transform the correlated variables to create a new composite variable.\n",
    "# 3. Collect more data to reduce the correlation between the variables.\n",
    "# 4. Use regularization techniques like ridge regression or lasso regression, which can help mitigate the impact of multicollinearity.\n",
    "\n",
    "# By detecting and addressing multicollinearity, one can improve the reliability and interpretability of the multiple linear regression model by ensuring that the independent variables contribute independently to the prediction of the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e17a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.\n",
    "\n",
    "# Polynomial regression is an extension of linear regression that allows for modeling nonlinear relationships between the dependent variable and the independent variable(s).\n",
    "# While linear regression assumes a linear relationship, polynomial regression can capture more complex curves and patterns by including polynomial terms in the model equation.\n",
    "\n",
    "# In polynomial regression, the model equation takes the form:\n",
    "\n",
    "# hθ(x) = θ0 +  θ1X + θ2X² + ... + θnXⁿ\n",
    "\n",
    "# Here, hθ(x) represents the dependent variable, X represents the independent variable, θ0, θ1, θ2, ..., θn are the coefficients for the polynomial terms.\n",
    "\n",
    "# The key difference between polynomial regression and linear regression is that polynomial regression can model curved relationships, whereas linear regression assumes a straight-line relationship.\n",
    "# By including higher-order polynomial terms (e.g., X², X³, etc.), polynomial regression can capture nonlinear patterns in the data, allowing for a more flexible and accurate representation of the relationship between the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.\n",
    "\n",
    "# Advantages of polynomial regression compared to linear regression include the ability to capture nonlinear relationships between variables and provide more flexible curve fitting. It can better model complex patterns and variations in the data, allowing for improved prediction accuracy.\n",
    "\n",
    "# However, polynomial regression has some disadvantages. It can lead to overfitting, particularly when higher-order polynomial terms are included, which may result in poor generalization to new data. Additionally, interpreting the coefficients in polynomial regression becomes more complex compared to linear regression.\n",
    "\n",
    "# Polynomial regression is preferred when there is evidence of nonlinear relationships in the data. It is useful when the underlying relationship between variables is expected to be curvilinear. Polynomial regression can be valuable in fields like physics, engineering, and social sciences, where relationships may exhibit nonlinear behavior. However, caution should be exercised to avoid overfitting by selecting an appropriate degree of the polynomial and using validation techniques to assess model performance on new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
