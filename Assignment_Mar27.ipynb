{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec0b7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "\n",
    "# R-squared, also known as the coefficient of determination, is a statistical measure used in linear regression models to assess the goodness of fit.\n",
    "# It indicates the proportion of the dependent variable's variance that can be explained by the independent variables included in the model.\n",
    "\n",
    "# R-squared is calculated by subtracting the ratio of the residual sum of squares (RSS) to the total sum of squares (TSS).\n",
    "# RSS measures the variability explained by the average of y line, while TSS represents the total variability in the dependent variable.\n",
    "\n",
    "# R_squared = 1 - ( RSS / TSS )\n",
    "\n",
    "# The R-squared value ranges from 0 to 1, where 0 indicates that the independent variables have no explanatory power, and 1 suggests a perfect fit, where all the variation in the dependent variable is explained by the independent variables.\n",
    "\n",
    "# It's important to note that R-squared alone does not provide information about the model's reliability or the significance of the independent variables, so other statistical measures should be considered alongside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db073b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "\n",
    "# Adjusted R-squared is a modification of the regular R-squared that accounts for the number of predictors or independent variables in a linear regression model.\n",
    "# While R-squared measures the proportion of the dependent variable's variance explained by the predictors, adjusted R-squared considers the complexity of the model and penalizes the inclusion of unnecessary variables.\n",
    "\n",
    "# R_squared_adjusted = 1 - ((1 - R_squared)*(N - 1)/(N - p - 1))\n",
    "\n",
    "# Where,\n",
    "# N = total number of datapoints\n",
    "# p = number of independent features\n",
    "\n",
    "# Adjusted R-squared is calculated by adjusting the R-squared value based on the number of predictors and the sample size.\n",
    "# It increases only if the additional predictors improve the model significantly, while it decreases if the added predictors do not contribute enough explanatory power.\n",
    "\n",
    "# In contrast to R-squared, adjusted R-squared takes into account model complexity and guards against overfitting.\n",
    "# It provides a more conservative and reliable measure of the model's goodness of fit, particularly when comparing models with different numbers of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ef9794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.\n",
    "\n",
    "# Adjusted R-squared is more appropriate to use when comparing and evaluating models with different numbers of predictors or independent variables.\n",
    "# It accounts for the complexity of the model and adjusts the R-squared value accordingly.\n",
    "# This makes it particularly useful in situations where there is a trade-off between model complexity and the number of predictors.\n",
    "\n",
    "# Adjusted R-squared helps to address the issue of overfitting, which occurs when a model performs well on the training data but fails to generalize to new data.\n",
    "# By penalizing the inclusion of unnecessary variables, adjusted R-squared discourages the addition of predictors that do not contribute significantly to the model's explanatory power.\n",
    "\n",
    "# Therefore, adjusted R-squared is valuable when selecting the most appropriate model among several competing models.\n",
    "# As it helps to ensure a balance between model complexity and goodness of fit, providing a more reliable measure for model comparison and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06ca0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.\n",
    "\n",
    "# MSE (Mean Squared Error): \n",
    "# MSE is calculated by taking the average of the squared differences between the predicted values and the actual values.\n",
    "# It represents the average of the squared errors and provides a measure of the overall model fit.\n",
    "# However, since it is calculated using squared errors, it is sensitive to outliers.\n",
    "# MSE = ∑(y_actual - y_predicted)²/n\n",
    "\n",
    "# RMSE (Root Mean Squared Error):\n",
    "# RMSE is the square root of MSE.\n",
    "# It measures the average magnitude of the residuals and is in the same units as the dependent variable.\n",
    "# RMSE is often preferred as it gives more intuitive and interpretable results.\n",
    "# RMSE =  √(MSE) = √(∑(y_actual - y_predicted)²/n)\n",
    "\n",
    "# MAE (Mean Absolute Error):\n",
    "# MAE is calculated by taking the average of the absolute differences between the predicted values and the actual values.\n",
    "# It represents the average of the absolute errors and is less sensitive to outliers compared to MSE.\n",
    "# MAE is useful when the absolute magnitude of errors is important.\n",
    "# MAE = ∑|y_actual - y_predicted|/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c9e2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "\n",
    "# MSE:\n",
    "# Advantage -\n",
    "# a. Equation is differentiable.\n",
    "# b. It has only one local or global minima.\n",
    "# Disadvantage -\n",
    "# a. Not robust to the outliers.\n",
    "# b. It don't have same unit.\n",
    "\n",
    "# RMSE:\n",
    "# Advantage - \n",
    "# a. It has same unit.\n",
    "# b. Equation is differentiable.\n",
    "# Disadvantage -\n",
    "# a. Not robust to outliers.\n",
    "\n",
    "# MAE:\n",
    "# Advantage -\n",
    "# a. Robust to outliers.\n",
    "# b. It has same unit.\n",
    "# Disadvantage -\n",
    "# a. Usually, convergence takes more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386aa434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.\n",
    "\n",
    "# Lasso regularization, also known as L1 regularization, is a technique used in machine learning to reduce overfitting and improve model performance.\n",
    "# It achieves this by adding a penalty term to the loss function that encourages the model to have sparse feature weights, effectively performing feature selection.\n",
    "\n",
    "# The key difference between Lasso regularization and Ridge regularization (L2 regularization) lies in the penalty term.\n",
    "# Lasso regularization adds the absolute values of the coefficients as the penalty, while Ridge regularization adds the squared values of the coefficients.\n",
    "# As a result, Lasso regularization tends to shrink some coefficients to exactly zero, effectively eliminating those features from the model, whereas Ridge regularization only reduces the magnitudes of the coefficients.\n",
    "\n",
    "# Lasso regularization is more appropriate when there is a belief that only a subset of the features is truly important for the model's predictive performance.\n",
    "# By setting irrelevant or less important coefficients to zero, Lasso helps in feature selection and simplifying the model.\n",
    "# This can be particularly useful in situations where the dataset has a large number of features or when interpretability is important.\n",
    "# However, Lasso regularization can be sensitive to correlated features, and in such cases, Ridge regularization might be a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b13bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.\n",
    "\n",
    "# Regularized linear models help prevent overfitting in machine learning by introducing a penalty term to the loss function.\n",
    "# This penalty term discourages the model from assigning excessively large weights to the features, thereby reducing the complexity of the model and preventing it from fitting the noise in the training data too closely.\n",
    "\n",
    "# For example, let's consider a linear regression problem where we have a dataset with 100 features and 1000 data points.\n",
    "# Without regularization, the model may be tempted to assign high weights to all the features, even those that are not truly relevant for making accurate predictions.\n",
    "# This can lead to overfitting, where the model becomes too specific to the training data and performs poorly on unseen data.\n",
    "\n",
    "# By applying regularization, such as Ridge or Lasso, the model is encouraged to shrink the weights of less important features or set them exactly to zero.\n",
    "# This regularization reduces the model's complexity and prevents it from relying too heavily on individual features, resulting in a more generalizable and less overfit model that performs better on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc36327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8.\n",
    "\n",
    "# While regularized linear models offer valuable benefits in preventing overfitting, they do have limitations that make them not always the best choice for regression analysis.\n",
    "\n",
    "# 1. Linearity Assumption:\n",
    "# Regularized linear models assume a linear relationship between the features and the target variable.\n",
    "# If the relationship is nonlinear, these models may not capture the underlying patterns effectively, leading to suboptimal performance.\n",
    "\n",
    "# 2. Feature Correlation:\n",
    "# Regularization methods like Ridge and Lasso can struggle with highly correlated features.\n",
    "# In such cases, it becomes challenging to determine which features to select or penalize, as they provide redundant information.\n",
    "# This can lead to instability and difficulties in interpretation.\n",
    "\n",
    "# 3. Model Complexity:\n",
    "# Regularized linear models may not be suitable when the relationship between the features and the target variable is highly complex.\n",
    "# In these situations, more flexible models like decision trees, random forests, or neural networks might provide better performance.\n",
    "\n",
    "# 4. Interpretability:\n",
    "# While regularization helps with feature selection, it can make the resulting model less interpretable.\n",
    "# Setting some feature coefficients to zero or shrinking them might make it challenging to understand the specific role and impact of each feature on the predictions.\n",
    "\n",
    "# 5. Outliers:\n",
    "# Regularized linear models are sensitive to outliers.\n",
    "# Outliers can disproportionately influence the penalty term, leading to biased model estimates.\n",
    "# Robust regression techniques or alternative models may be more appropriate in the presence of outliers.\n",
    "\n",
    "# Considering these limitations, it is important to carefully evaluate the nature of the data.\n",
    "# The relationship between features and the target variable, and the desired interpretability before deciding on the use of regularized linear models for regression analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994a312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.\n",
    "\n",
    "# In this scenario, we are comparing Model A and Model B based on their evaluation metrics.\n",
    "# Model A has an RMSE of 10, while Model B has an MAE of 8. \n",
    "\n",
    "# To determine which model is better, we need to consider the specific context and requirements of the problem. \n",
    "\n",
    "# If we prioritize penalizing larger errors and want to emphasize the impact of outliers, Model A with a lower RMSE of 10 would be preferred.\n",
    "# RMSE puts more weight on larger errors, making it sensitive to outliers.\n",
    "\n",
    "# On the other hand, if we prioritize a metric that is robust to outliers and want to focus on the overall average error, Model B with a lower MAE of 8 would be preferred.\n",
    "# MAE treats errors equally regardless of their magnitude and is not affected by outliers.\n",
    "\n",
    "# It's important to note that the choice of the evaluation metric is subjective and should be aligned with the problem's objectives.\n",
    "# Additionally, the limitations of the chosen metric should be considered.\n",
    "# For example, RMSE and MAE do not provide information on the direction of errors, and they both have their own biases and interpretations.\n",
    "# Therefore, it is crucial to carefully assess the limitations and context of the problem before making a decision solely based on the evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca20c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10.\n",
    "\n",
    "# Choosing the better performer between Model A (Ridge regularization with a regularization parameter of 0.1) and Model B (Lasso regularization with a regularization parameter of 0.5) depends on the specific characteristics of the problem at hand.\n",
    "\n",
    "# Ridge regularization (L2 regularization) tends to shrink the coefficients towards zero without necessarily setting them exactly to zero. This makes it suitable when there is a belief that all the features contribute to the model's predictive performance to some extent. It helps to reduce overfitting and handle multicollinearity. Model A, with Ridge regularization, may be preferred when the dataset has correlated features and all features are considered important.\n",
    "\n",
    "# On the other hand, Lasso regularization (L1 regularization) encourages sparse feature weights and performs feature selection by setting some coefficients to exactly zero. It is more appropriate when there is a belief that only a subset of features is truly important for the model's performance. Model B, with Lasso regularization, may be preferred when the dataset has many irrelevant or redundant features, and interpretability or feature selection is important.\n",
    "\n",
    "# However, it's important to note the trade-offs and limitations of each regularization method. Ridge regularization can handle correlated features but may not effectively eliminate irrelevant features. Lasso regularization performs feature selection but can struggle with highly correlated features. It may also select only one feature from a group of correlated features arbitrarily. Therefore, careful consideration should be given to the specific characteristics and requirements of the problem when choosing the better performer and the appropriate regularization method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
